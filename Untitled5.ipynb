{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0a0e4b",
   "metadata": {},
   "source": [
    "### Econmetric Python Lab Assignment 2\n",
    "\n",
    "#### Huitong Li (606353219)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5448af",
   "metadata": {},
   "source": [
    "### 1.Using the data in the file toody5, estimate the model\n",
    "\n",
    "\\begin{align*}\n",
    "Y_t = \\beta_1 + \\beta_2 \\cdot \\text{TREND}_t + \\beta_3 \\cdot \\text{RAIN}_t + \\beta_4 \\cdot \\text{RAIN}_t^2 + \\beta_5 \\cdot (\\text{RAIN}_t \\times \\text{TREND}_t) + e_t \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "abdb633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.diagnostic import linear_reset\n",
    "import wooldridge as woo \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan \n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'path_to_data/toody5.csv' with the actual path of your dataset\n",
    "df1 = pd.read_csv('toody5-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d60c6be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateid01</th>\n",
       "      <th>rain</th>\n",
       "      <th>t</th>\n",
       "      <th>trend</th>\n",
       "      <th>y</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/1950</td>\n",
       "      <td>4.099</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3508</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/1951</td>\n",
       "      <td>3.574</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9476</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/1952</td>\n",
       "      <td>4.258</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2569</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/1953</td>\n",
       "      <td>4.790</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9142</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/1954</td>\n",
       "      <td>3.648</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1022</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dateid01   rain  t  trend       y  year\n",
       "0  1/1/1950  4.099  1    0.0  1.3508  1950\n",
       "1  1/1/1951  3.574  2    0.1  0.9476  1951\n",
       "2  1/1/1952  4.258  3    0.2  1.2569  1952\n",
       "3  1/1/1953  4.790  4    0.3  0.9142  1953\n",
       "4  1/1/1954  3.648  5    0.4  1.1022  1954"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f5f399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rain</th>\n",
       "      <th>t</th>\n",
       "      <th>trend</th>\n",
       "      <th>y</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>48.00</td>\n",
       "      <td>48.000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>48.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.035188</td>\n",
       "      <td>24.50</td>\n",
       "      <td>2.350</td>\n",
       "      <td>1.476737</td>\n",
       "      <td>1973.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.872232</td>\n",
       "      <td>14.00</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0.542493</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.052000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.362900</td>\n",
       "      <td>1950.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.372250</td>\n",
       "      <td>12.75</td>\n",
       "      <td>1.175</td>\n",
       "      <td>1.088850</td>\n",
       "      <td>1961.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.835500</td>\n",
       "      <td>24.50</td>\n",
       "      <td>2.350</td>\n",
       "      <td>1.352300</td>\n",
       "      <td>1973.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.678000</td>\n",
       "      <td>36.25</td>\n",
       "      <td>3.525</td>\n",
       "      <td>1.738125</td>\n",
       "      <td>1985.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.097000</td>\n",
       "      <td>48.00</td>\n",
       "      <td>4.700</td>\n",
       "      <td>2.966900</td>\n",
       "      <td>1997.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rain      t   trend          y     year\n",
       "count  48.000000  48.00  48.000  48.000000    48.00\n",
       "mean    4.035188  24.50   2.350   1.476737  1973.50\n",
       "std     0.872232  14.00   1.400   0.542493    14.00\n",
       "min     2.052000   1.00   0.000   0.362900  1950.00\n",
       "25%     3.372250  12.75   1.175   1.088850  1961.75\n",
       "50%     3.835500  24.50   2.350   1.352300  1973.50\n",
       "75%     4.678000  36.25   3.525   1.738125  1985.25\n",
       "max     6.097000  48.00   4.700   2.966900  1997.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5da32",
   "metadata": {},
   "source": [
    "### 1. (a) Report your estimates, standard errors, t-values, and p-values in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7cb7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "toody_mod = smf.ols('y ~ trend + rain + I(rain**2) + rain:trend',\n",
    "                    data=df1).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360f5b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.726\n",
      "Model:                            OLS   Adj. R-squared:                  0.701\n",
      "Method:                 Least Squares   F-statistic:                     28.54\n",
      "Date:                Tue, 05 Nov 2024   Prob (F-statistic):           1.32e-11\n",
      "Time:                        21:35:37   Log-Likelihood:                -7.1445\n",
      "No. Observations:                  48   AIC:                             24.29\n",
      "Df Residuals:                      43   BIC:                             33.64\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept       -2.6747      0.997     -2.684      0.010      -4.684      -0.665\n",
      "trend            0.6781      0.192      3.533      0.001       0.291       1.065\n",
      "rain             1.5082      0.431      3.501      0.001       0.640       2.377\n",
      "I(rain ** 2)    -0.1571      0.047     -3.369      0.002      -0.251      -0.063\n",
      "rain:trend      -0.0928      0.047     -1.985      0.054      -0.187       0.001\n",
      "==============================================================================\n",
      "Omnibus:                        2.460   Durbin-Watson:                   1.031\n",
      "Prob(Omnibus):                  0.292   Jarque-Bera (JB):                1.542\n",
      "Skew:                           0.171   Prob(JB):                        0.463\n",
      "Kurtosis:                       2.191   Cond. No.                         538.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(toody_mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a7debb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>t-value</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-2.674652</td>\n",
       "      <td>0.996569</td>\n",
       "      <td>-2.683862</td>\n",
       "      <td>0.010292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trend</th>\n",
       "      <td>0.678101</td>\n",
       "      <td>0.191928</td>\n",
       "      <td>3.533095</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain</th>\n",
       "      <td>1.508177</td>\n",
       "      <td>0.430729</td>\n",
       "      <td>3.501450</td>\n",
       "      <td>0.001092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I(rain ** 2)</th>\n",
       "      <td>-0.157079</td>\n",
       "      <td>0.046622</td>\n",
       "      <td>-3.369233</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rain:trend</th>\n",
       "      <td>-0.092838</td>\n",
       "      <td>0.046761</td>\n",
       "      <td>-1.985389</td>\n",
       "      <td>0.053502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Estimate  Std. Error   t-value   p-value\n",
       "Intercept    -2.674652    0.996569 -2.683862  0.010292\n",
       "trend         0.678101    0.191928  3.533095  0.000996\n",
       "rain          1.508177    0.430729  3.501450  0.001092\n",
       "I(rain ** 2) -0.157079    0.046622 -3.369233  0.001600\n",
       "rain:trend   -0.092838    0.046761 -1.985389  0.053502"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the results into a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Estimate\": toody_mod.params,\n",
    "    \"Std. Error\": toody_mod.bse,\n",
    "    \"t-value\": toody_mod.tvalues,\n",
    "    \"p-value\": toody_mod.pvalues\n",
    "})\n",
    "\n",
    "# Displaying the table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44ffb0",
   "metadata": {},
   "source": [
    "### 1. (b) Do the coefficients have the expected signs? Why? (One of the objectives of technological improvements is the development of drought-resistant varieties of wheat.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664c874",
   "metadata": {},
   "source": [
    "#### Intercept: \n",
    "\n",
    "The intercept coefficient is -2.6747. This represents the estimated wheat yield when all other variables are zero. Although it’s unlikely to have all other factors (trend, rain, etc.) at zero, this negative intercept suggests that without the contributing factors of rainfall and technological improvements, wheat yield would be unsustainable or negative, which is logical in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee6fed",
   "metadata": {},
   "source": [
    "#### Trend (β2 = 0.6781):\n",
    "\n",
    "The positive coefficient for the trend variable aligns with the expected outcome because trend is intended to capture technological progress over time. As technology improves (e.g., better agricultural techniques, machinery, or drought-resistant crop varieties), the wheat yield is expected to increase over time. This positive relationship suggests that advancements are positively impacting productivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09840d20",
   "metadata": {},
   "source": [
    "#### Rain (β3 = 1.5082):\n",
    "\n",
    "The positive coefficient for rain suggests that as rainfall increases, wheat yield also increases, which is expected since water is essential for crop growth. This coefficient indicates that a higher amount of rainfall positively impacts wheat production, assuming other conditions are controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6085e18",
   "metadata": {},
   "source": [
    "#### Rain^2 (β4 = -0.1571): \n",
    "\n",
    "The negative coefficient for the squared term of rainfall suggests a diminishing return effect of rainfall on wheat yield. Initially, rainfall positively impacts yield, but excessive rain beyond a certain level could negatively impact productivity, possibly due to waterlogging or other adverse effects. This quadratic relationship (positive linear term and negative squared term) is consistent with the expectation that there is an optimal amount of rain for crop growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e74492",
   "metadata": {},
   "source": [
    "#### Interaction between Rain and Trend (β5 = -0.0928):\n",
    "\n",
    "The negative coefficient for the interaction term between rain and trend may suggest that as technology improves, the dependency of wheat yield on rainfall decreases. In other words, technological advances (like drought-resistant crops) might reduce the yield's sensitivity to variations in rainfall. This aligns with the context of technological improvements aimed at making crops more resilient to environmental factors, such as irregular rainfall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b49faa",
   "metadata": {},
   "source": [
    "#### In summary:\n",
    "\n",
    "The signs of the coefficients for trend and rain are as expected, reflecting that both technological progress and increased rainfall up to a certain level positively impact wheat yield.\n",
    "\n",
    "The negative sign for rain squared indicates diminishing returns or potential negative effects of excessive rainfall.\n",
    "\n",
    "The interaction term also has an expected negative sign, suggesting technological improvements may reduce the crop’s dependence on rainfall, consistent with the objective of developing drought-resistant wheat varieties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbadd8a0",
   "metadata": {},
   "source": [
    "### 1.(c) Find point and 95% interval estimates of the marginal effect of extra rainfall in (i) 1959 when the rainfall was 2.98 dm and (ii) 1995 when the rainfall was 4.797 dm. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b87b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959:\n",
      "Marginal Effect Point Estimate: 0.48843078125789413\n",
      "95% Confidence Interval: (0.2030999605639356, 0.7737616019518527)\n",
      "\n",
      "1995:\n",
      "Marginal Effect Point Estimate: -0.4166123558129174\n",
      "95% Confidence Interval: (-0.7196625434501382, -0.11356216817569659)\n"
     ]
    }
   ],
   "source": [
    "# Values for the trend and rainfall in the years 1959 and 1995\n",
    "trend_1959 = 0.9 #as it is 0 from 1950, and add 0.1 for one year more\n",
    "rain_1959 = 2.98\n",
    "trend_1995 = 4.5 #the same\n",
    "rain_1995 = 4.797\n",
    "\n",
    "# Extract coefficients and covariance matrix from the model\n",
    "coef = toody_mod.params\n",
    "cov = toody_mod.cov_params()\n",
    "\n",
    "# Marginal effect formula for each year\n",
    "def marginal_effect(trend, rain):\n",
    "    return coef['rain'] + 2 * coef['I(rain ** 2)'] * rain + coef['rain:trend'] * trend\n",
    "\n",
    "# Calculate point estimates\n",
    "me_1959 = marginal_effect(trend_1959, rain_1959)\n",
    "me_1995 = marginal_effect(trend_1995, rain_1995)\n",
    "\n",
    "# Variance of the marginal effect for confidence intervals\n",
    "var_1959 = (cov.loc['rain', 'rain'] +\n",
    "            (2 * rain_1959) ** 2 * cov.loc['I(rain ** 2)', 'I(rain ** 2)'] +\n",
    "            (trend_1959) ** 2 * cov.loc['rain:trend', 'rain:trend'] +\n",
    "            2 * (2 * rain_1959) * cov.loc['rain', 'I(rain ** 2)'] +\n",
    "            2 * trend_1959 * cov.loc['rain', 'rain:trend'] +\n",
    "            2 * (2 * rain_1959) * trend_1959 * cov.loc['I(rain ** 2)', 'rain:trend'])\n",
    "\n",
    "var_1995 = (cov.loc['rain', 'rain'] +\n",
    "            (2 * rain_1995) ** 2 * cov.loc['I(rain ** 2)', 'I(rain ** 2)'] +\n",
    "            (trend_1995) ** 2 * cov.loc['rain:trend', 'rain:trend'] +\n",
    "            2 * (2 * rain_1995) * cov.loc['rain', 'I(rain ** 2)'] +\n",
    "            2 * trend_1995 * cov.loc['rain', 'rain:trend'] +\n",
    "            2 * (2 * rain_1995) * trend_1995 * cov.loc['I(rain ** 2)', 'rain:trend'])\n",
    "\n",
    "# Standard errors\n",
    "se_1959 = np.sqrt(var_1959)\n",
    "se_1995 = np.sqrt(var_1995)\n",
    "\n",
    "# 95% confidence intervals\n",
    "ci_1959 = (me_1959 - 1.96 * se_1959, me_1959 + 1.96 * se_1959)\n",
    "ci_1995 = (me_1995 - 1.96 * se_1995, me_1995 + 1.96 * se_1995)\n",
    "\n",
    "# Display results\n",
    "print(\"1959:\")\n",
    "print(\"Marginal Effect Point Estimate:\", me_1959)\n",
    "print(\"95% Confidence Interval:\", ci_1959)\n",
    "\n",
    "print(\"\\n1995:\")\n",
    "print(\"Marginal Effect Point Estimate:\", me_1995)\n",
    "print(\"95% Confidence Interval:\", ci_1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314f41d",
   "metadata": {},
   "source": [
    "#### Interpretation of Results\n",
    "\n",
    "#### 1959:\n",
    "\n",
    "Marginal Effect Point Estimate: 0.4884\n",
    "\n",
    "95% Confidence Interval: (0.2031, 0.7738)\n",
    "\n",
    "The positive point estimate of 0.4884 suggests that in 1959, an additional unit of rainfall (decimeter) was associated with an increase in wheat yield. The confidence interval (0.2031, 0.7738) does not include zero, indicating that this positive effect is statistically significant at the 5% level. This could imply that during this period, additional rainfall positively impacted wheat production, likely due to the reliance on natural rainfall for crop growth.\n",
    "\n",
    "#### 1995:\n",
    "\n",
    "Marginal Effect Point Estimate: -0.4166\n",
    "\n",
    "95% Confidence Interval: (-0.7197, -0.1136)\n",
    "\n",
    "The negative point estimate of -0.4166 for 1995 indicates that an additional unit of rainfall was associated with a decrease in wheat yield. The confidence interval (-0.7197, -0.1136) also does not include zero, suggesting that this negative effect is statistically significant at the 5% level. This change in the impact of rainfall from positive in 1959 to negative in 1995 could be attributed to technological advancements, such as the development of drought-resistant varieties, or changes in farming practices, which made yield less dependent on or even adversely affected by excess rainfall.\n",
    "\n",
    "#### Discussion\n",
    "The contrasting results between 1959 and 1995 highlight an interesting shift in the relationship between rainfall and wheat yield. In 1959, additional rainfall had a positive impact, possibly due to traditional agricultural practices and the dependency on rainfall. By 1995, however, the negative effect suggests that either too much rainfall became detrimental (perhaps due to waterlogging or nutrient leaching) or that advancements in agricultural technology made optimal yield less dependent on high rainfall, possibly due to irrigation, drought-resistant crops, or other improvements.\n",
    "\n",
    "The shift from a positive to a negative marginal effect of rainfall over time supports the hypothesis that technological improvements have successfully developed wheat varieties better suited to withstand periods of lower rainfall. This evolution would reduce reliance on rainfall for optimal yields and potentially increase resilience against climatic variability.\n",
    "\n",
    "This analysis provides insight into how agricultural technology and crop varieties have evolved to adapt to or mitigate natural environmental conditions over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6439d9",
   "metadata": {},
   "source": [
    "### 2. A study claims that wages exhibit two key marginal effects:\n",
    "\n",
    "1. The marginal effect of experience: one additional year increases wage by 0.8%\n",
    "\n",
    "2. The marginal effect of education: one additional year has the same effect as 14 years of experience, meaning it increases wage by 14 × 0.8% = 11.2%\n",
    "\n",
    "Using the cps5 small dataset (restricted to observations with more than 7 years of education), we will test these claims about marginal effects at the 5% significance level. Note that since our dependent variable is log(wage), the coefficients will directly represent the claimed percentage changes (0.8% = 0.008 and 11.2% = 0.112)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4629c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df2 = pd.read_csv('cps5_small-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0449505e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>educ</th>\n",
       "      <th>exper</th>\n",
       "      <th>faminc</th>\n",
       "      <th>female</th>\n",
       "      <th>metro</th>\n",
       "      <th>midwest</th>\n",
       "      <th>south</th>\n",
       "      <th>wage</th>\n",
       "      <th>west</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>45351</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>91946</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>42</td>\n",
       "      <td>48370</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  educ  exper  faminc  female  metro  midwest  south   wage  west\n",
       "0      0    13     45       0       1      1        0      0  44.44     1\n",
       "1      0    14     25   45351       1      1        1      0  16.00     0\n",
       "2      0    18     27   91946       1      1        0      0  15.38     0\n",
       "3      0    13     42   48370       0      1        1      0  13.54     0\n",
       "4      0    13     41   10000       1      1        0      0  25.00     1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f69c934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>educ</th>\n",
       "      <th>exper</th>\n",
       "      <th>faminc</th>\n",
       "      <th>female</th>\n",
       "      <th>metro</th>\n",
       "      <th>midwest</th>\n",
       "      <th>south</th>\n",
       "      <th>wage</th>\n",
       "      <th>west</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.00000</td>\n",
       "      <td>1200.00000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.087500</td>\n",
       "      <td>14.202500</td>\n",
       "      <td>23.374167</td>\n",
       "      <td>35304.421667</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.24750</td>\n",
       "      <td>0.32500</td>\n",
       "      <td>23.640042</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.282684</td>\n",
       "      <td>2.890811</td>\n",
       "      <td>13.269296</td>\n",
       "      <td>45026.488224</td>\n",
       "      <td>0.496594</td>\n",
       "      <td>0.382953</td>\n",
       "      <td>0.43174</td>\n",
       "      <td>0.46857</td>\n",
       "      <td>15.216554</td>\n",
       "      <td>0.434628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>23679.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>53029.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>469000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>221.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             black         educ        exper         faminc       female  \\\n",
       "count  1200.000000  1200.000000  1200.000000    1200.000000  1200.000000   \n",
       "mean      0.087500    14.202500    23.374167   35304.421667     0.440000   \n",
       "std       0.282684     2.890811    13.269296   45026.488224     0.496594   \n",
       "min       0.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "25%       0.000000    12.000000    12.000000       0.000000     0.000000   \n",
       "50%       0.000000    14.000000    24.000000   23679.000000     0.000000   \n",
       "75%       0.000000    16.000000    34.000000   53029.000000     1.000000   \n",
       "max       1.000000    21.000000    62.000000  469000.000000     1.000000   \n",
       "\n",
       "             metro     midwest       south         wage         west  \n",
       "count  1200.000000  1200.00000  1200.00000  1200.000000  1200.000000  \n",
       "mean      0.821667     0.24750     0.32500    23.640042     0.252500  \n",
       "std       0.382953     0.43174     0.46857    15.216554     0.434628  \n",
       "min       0.000000     0.00000     0.00000     3.940000     0.000000  \n",
       "25%       1.000000     0.00000     0.00000    13.000000     0.000000  \n",
       "50%       1.000000     0.00000     0.00000    19.300000     0.000000  \n",
       "75%       1.000000     0.00000     1.00000    29.800000     1.000000  \n",
       "max       1.000000     1.00000     1.00000   221.100000     1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d0345",
   "metadata": {},
   "source": [
    "#### 2. (a)  Estimate the model:\n",
    "\n",
    "\\begin{align*}\n",
    "ln(\\text{WAGE}) = \\beta_1 + \\beta_2 \\text{EDUC} + \\beta_3 \\text{EXPER} + e\n",
    "\\end{align*}\n",
    "\n",
    "and jointly test the claim above about the marginal effects of EDUC and EXPER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84b63cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.297\n",
      "Model:                            OLS   Adj. R-squared:                  0.296\n",
      "Method:                 Least Squares   F-statistic:                     252.7\n",
      "Date:                Tue, 05 Nov 2024   Prob (F-statistic):           2.74e-92\n",
      "Time:                        22:47:45   Log-Likelihood:                -800.10\n",
      "No. Observations:                1200   AIC:                             1606.\n",
      "Df Residuals:                    1197   BIC:                             1621.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.2834      0.078     16.376      0.000       1.130       1.437\n",
      "educ           0.1067      0.005     22.167      0.000       0.097       0.116\n",
      "exper          0.0086      0.001      8.169      0.000       0.007       0.011\n",
      "==============================================================================\n",
      "Omnibus:                        3.122   Durbin-Watson:                   1.981\n",
      "Prob(Omnibus):                  0.210   Jarque-Bera (JB):                3.205\n",
      "Skew:                           0.064   Prob(JB):                        0.201\n",
      "Kurtosis:                       3.218   Cond. No.                         171.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Fit the linear model\n",
    "wage_model = smf.ols('np.log(wage) ~ educ + exper', data=df2).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(wage_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec5164",
   "metadata": {},
   "source": [
    "#### 1. Model Fit\n",
    "\n",
    "R-squared: 0.297 indicates that approximately 29.7% of the variance in the log of wages is explained by the model's predictors (education and experience). This is a moderate fit, suggesting other factors may also influence wages.\n",
    "\n",
    "Adjusted R-squared: 0.296, slightly lower than R-squared, accounts for the number of predictors in the model and suggests similar explanatory power when adjusted for model complexity.\n",
    "\n",
    "F-statistic: 252.7 with a p-value of 2.74e-92 indicates that the overall model is statistically significant, meaning at least one of the predictors (education or experience) has a significant relationship with the dependent variable (log of wage).\n",
    "\n",
    "#### 2. Coefficients\n",
    "\n",
    "Each coefficient represents the estimated change in the log of wages for a one-unit increase in the predictor, holding other predictors constant.\n",
    "\n",
    "Intercept: 1.2834 (p < 0.001) is statistically significant. This is the estimated log wage when both education and experience are zero, although this value may not have a meaningful interpretation in this context as few people with zero years of education and experience would be in the dataset.\n",
    "\n",
    "Education (educ): 0.1067 (p < 0.001) is statistically significant, indicating that each additional year of education is associated with a 10.67% increase in wages (since we are using log(wage), the coefficient can be interpreted as a percentage change).\n",
    "\n",
    "Experience (exper): 0.0086 (p < 0.001) is also statistically significant, meaning each additional year of experience is associated with an approximate 0.86% increase in wages.\n",
    "\n",
    "#### 3. Statistical Tests\n",
    "\n",
    "t-values: Each coefficient's t-value indicates how many standard deviations the coefficient estimate is from zero. High t-values (like those observed here) suggest that the coefficients are significantly different from zero.\n",
    "\n",
    "p-values: All p-values are below 0.001, indicating that the intercept, education, and experience are all statistically significant predictors of log wages at common significance levels (e.g., 5%).\n",
    "\n",
    "#### 4. Confidence Intervals\n",
    "\n",
    "The 95% confidence intervals for each coefficient suggest the range within which the true parameter values are likely to fall:\n",
    "\n",
    "Education: [0.097, 0.116]\n",
    "Experience: [0.007, 0.011]\n",
    "\n",
    "These intervals do not include zero, reaffirming the significance of the predictors.\n",
    "\n",
    "#### 5. Diagnostic Tests\n",
    "\n",
    "Omnibus, Jarque-Bera, Skew, Kurtosis: These tests assess the normality of the residuals. The Omnibus and Jarque-Bera test p-values (0.210 and 0.201, respectively) are above 0.05, suggesting no strong evidence against the normality of residuals.\n",
    "\n",
    "Durbin-Watson: 1.981, close to 2, suggests no significant autocorrelation in residuals, which is favorable for model assumptions.\n",
    "\n",
    "Condition Number: 171, which is relatively high, may indicate some multicollinearity, although this is not extreme.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The model significantly explains the variation in log wages based on education and experience, with both predictors having statistically significant positive effects on wages. The diagnostic tests indicate that model assumptions are reasonably met, with no issues in residual normality or autocorrelation. However, the R-squared value suggests there is still substantial unexplained variance in wages, indicating that additional predictors could potentially improve the model's explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6499a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=0.9047004870718919, p=0.4049395846225009, df_denom=1.2e+03, df_num=2>\n"
     ]
    }
   ],
   "source": [
    "# Joint hypothesis test\n",
    "# Testing if β2 = 0.112 and β3 = 0.008\n",
    "hypothesis = 'educ = 0.112, exper = 0.008'\n",
    "joint_test = wage_model.f_test(hypothesis)\n",
    "print(joint_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d37bc0e",
   "metadata": {},
   "source": [
    "The F-statistic of 0.905 is relatively low, and the p-value of 0.4049 is much higher than the typical significance level (e.g., 0.05).\n",
    "\n",
    "#### Conclusion\n",
    "Since the p-value (0.4049) is greater than 0.05, we fail to reject the null hypothesis. This result indicates that there is not enough evidence to conclude that the true values of β2 and β3 are different from the hypothesized values of 0.112 and 0.008, respectively.\n",
    "\n",
    "In simpler terms, the observed data does not provide sufficient evidence to dispute the claim that an additional year of education increases wages by 11.2% and an additional year of experience increases wages by 0.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d6a8b",
   "metadata": {},
   "source": [
    "### 2. (b) Use RESET to test the adequacy of the model; perform the test with the squares of the predictions and the squares and cubes of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32edb12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=19.072968972333086, p=7.017357398842768e-09, df_denom=1.2e+03, df_num=2>\n"
     ]
    }
   ],
   "source": [
    "print(linear_reset(wage_model, power=2, test_type='exog', use_f=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6423485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=15.645613102155853, p=1.747532108345629e-12, df_denom=1.19e+03, df_num=4>\n"
     ]
    }
   ],
   "source": [
    "print(linear_reset(wage_model, power=3, test_type='exog', use_f=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f7a97",
   "metadata": {},
   "source": [
    "#### Test with Squared Predictions (Power = 2)\n",
    "\n",
    "F-statistic: 19.073\n",
    "p-value: 7.018e-09\n",
    "Degrees of Freedom: df_denom is around 1200, and df_num is 2.\n",
    "The low p-value (significantly less than 0.05) indicates that the null hypothesis of correct model specification is rejected at a high level of significance. This suggests that including the square of the predictions significantly improves the model, indicating potential misspecification in the current linear model.\n",
    "\n",
    "#### Test with Squared and Cubed Predictions (Power = 3)\n",
    "\n",
    "F-statistic: 15.646\n",
    "p-value: 1.748e-12\n",
    "Degrees of Freedom: df_denom is around 1190, and df_num is 4.\n",
    "Similarly, the very low p-value (also far below 0.05) leads to rejecting the null hypothesis, implying that adding both the square and cube of the predictions provides additional explanatory power to the model. This result further suggests that the linear model may not fully capture the relationships in the data.\n",
    "\n",
    "#### Conclusion\n",
    "The results from both tests indicate that the current model may be misspecified and could benefit from a more complex functional form, potentially including non-linear terms. This implies that the relationships between the variables may not be strictly linear, and a better fit might be achieved by incorporating these higher-order terms into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1588f",
   "metadata": {},
   "source": [
    "### 2. (c) Suppose we have the model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln(\\text{WAGE}) = \\beta_1 + \\beta_2 \\text{EDUC} + \\beta_3 \\text{EXPER} + \\beta_4 \\text{EDUC}^2 + \\beta_5 \\text{EXPER}^2 + \\beta_6 (\\text{EDUC} \\times \\text{EXPER}) + e\n",
    "\\end{align*}\n",
    "\n",
    "#### Write the equation to calculate the marginal effect of education on the log of wage from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a60c7dbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.301\n",
      "Model:                            OLS   Adj. R-squared:                  0.299\n",
      "Method:                 Least Squares   F-statistic:                     128.8\n",
      "Date:                Tue, 05 Nov 2024   Prob (F-statistic):           1.85e-91\n",
      "Time:                        23:46:31   Log-Likelihood:                -796.45\n",
      "No. Observations:                1200   AIC:                             1603.\n",
      "Df Residuals:                    1195   BIC:                             1628.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        1.4628      0.271      5.397      0.000       0.931       1.995\n",
      "educ             0.0686      0.031      2.187      0.029       0.007       0.130\n",
      "exper            0.0133      0.005      2.519      0.012       0.003       0.024\n",
      "I(educ ** 2)     0.0017      0.001      1.862      0.063   -9.35e-05       0.004\n",
      "educ:exper      -0.0004      0.000     -0.954      0.340      -0.001       0.000\n",
      "==============================================================================\n",
      "Omnibus:                        5.110   Durbin-Watson:                   1.981\n",
      "Prob(Omnibus):                  0.078   Jarque-Bera (JB):                5.840\n",
      "Skew:                           0.065   Prob(JB):                       0.0539\n",
      "Kurtosis:                       3.316   Cond. No.                     8.53e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.53e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Fit the linear model\n",
    "wage_model2 = smf.ols('np.log(wage) ~ educ + exper + I(educ**2)+ educ:exper', data=df2).fit()\n",
    "\n",
    "# Display the model summary\n",
    "print(wage_model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe344a",
   "metadata": {},
   "source": [
    "#### Model Summary:\n",
    "\n",
    "Dependent Variable: The dependent variable here is np.log(wage), indicating that the model is predicting the natural logarithm of wage.\n",
    "\n",
    "R-squared: 0.301. This suggests that approximately 30.1% of the variance in the log of wages is explained by the model. This is a moderate level of explanatory power, meaning there are likely other factors not included in this model that affect wages.\n",
    "\n",
    "Adjusted R-squared: 0.299. Similar to R-squared but adjusted for the number of predictors in the model, providing a slightly more conservative measure of model fit.\n",
    "\n",
    "F-statistic: 128.8 with a p-value of 1.85e-91, indicating that the model as a whole is statistically significant and that at least one of the predictors contributes significantly to explaining variation in the dependent variable.\n",
    "\n",
    "#### Coefficients:\n",
    "\n",
    "Each predictor's effect on the log of wage is estimated by the coefficients, along with tests of statistical significance.\n",
    "\n",
    "Intercept (1.4628):\n",
    "Coefficient: 1.4628\n",
    "p-value: 0.000 (statistically significant at the 5% level)\n",
    "Interpretation: This is the estimated log wage when all other variables are zero, though in practical terms, this value alone might not hold substantive meaning without context.\n",
    "\n",
    "educ (0.0686):\n",
    "Coefficient: 0.0686\n",
    "p-value: 0.029 (statistically significant at the 5% level)\n",
    "Interpretation: An additional year of education is associated with an estimated 6.86% increase in wages, holding other variables constant, as indicated by the positive coefficient. This variable has a significant effect on wage.\n",
    "\n",
    "exper (0.0133):\n",
    "Coefficient: 0.0133\n",
    "p-value: 0.012 (statistically significant at the 5% level)\n",
    "Interpretation: Each additional year of experience is associated with an estimated 1.33% increase in wages, holding other variables constant. This positive coefficient indicates that experience contributes positively to wage.\n",
    "\n",
    "I(educ ** 2) (0.0017):\n",
    "Coefficient: 0.0017\n",
    "p-value: 0.063 (marginally significant, slightly above the 5% level)\n",
    "Interpretation: The quadratic term for education suggests a nonlinear effect. Since the coefficient is positive, it suggests that the effect of education on wages might increase as education increases, though the significance level is marginal.\n",
    "\n",
    "educ(-0.0004):\n",
    "Coefficient: -0.0004\n",
    "p-value: 0.340 (not statistically significant at the 5% level)\n",
    "Interpretation: This interaction term between education and experience is not statistically significant, indicating that there may not be a significant combined effect of education and experience on wage in this model.\n",
    "\n",
    "#### Additional Model Statistics:\n",
    "\n",
    "Omnibus, Prob(Omnibus), Skew, Kurtosis: These statistics provide information about the distribution of the residuals. Here, the residuals do not seem to deviate strongly from normality.\n",
    "\n",
    "Durbin-Watson (1.981): This value close to 2 suggests that there is little autocorrelation in the residuals.\n",
    "\n",
    "Jarque-Bera (JB) and Prob(JB): With a p-value of 0.0539, the test does not indicate a strong departure from normality in the residuals.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "The model is statistically significant overall, with education and experience showing significant effects on wages.\n",
    "\n",
    "The quadratic term for education has a marginally significant effect, suggesting a possible nonlinear relationship with wage.\n",
    "\n",
    "The interaction between education and experience is not statistically significant.\n",
    "\n",
    "The R-squared value indicates a moderate level of explanatory power, meaning other factors not included in this model likely influence wages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b206b6",
   "metadata": {},
   "source": [
    "#### Write the equation to calculate the marginal effect of education on the log of wage from this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70740db",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial \\ln(\\text{WAGE})}{\\partial \\text{EDUC}} = \\beta_2 + 2\\beta_4 \\text{EDUC} + \\beta_6 \\text{EXPER}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43825ee3",
   "metadata": {},
   "source": [
    "#### Write the equation to calculate the marginal effect of experience on the log of wage from this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6841591",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial \\ln(\\text{WAGE})}{\\partial \\text{EXPER}} = \\beta_3 + 2\\beta_5 \\text{EXPER} + \\beta_6 \\text{EDUC}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ac77d",
   "metadata": {},
   "source": [
    "### 2. (d) After estimating the model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln(\\text{WAGE}) = \\beta_1 + \\beta_2 \\text{EDUC} + \\beta_3 \\text{EXPER} + \\beta_4 \\text{EDUC}^2 + \\beta_5 \\text{EXPER}^2 + \\beta_6 (\\text{EDUC} \\times \\text{EXPER}) + e\n",
    "\\end{align*}\n",
    "\n",
    "jointly test the claims about the marginal effects of EDUC and EXPER at the following levels of EDUC and EXPER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baf2800e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For educ=10 and exper=5:\n",
      "  Marginal Effect of educ: 0.06683369390249881\n",
      "  Marginal Effect of exper: 0.06507198741553773\n",
      "\n",
      "For educ=14 and exper=24:\n",
      "  Marginal Effect of educ: 0.06013920925204667\n",
      "  Marginal Effect of exper: 0.06366262222596886\n",
      "\n",
      "For educ=18 and exper=40:\n",
      "  Marginal Effect of educ: 0.05450174849377117\n",
      "  Marginal Effect of exper: 0.06225325703639998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define values for EDUC and EXPER to evaluate marginal effects\n",
    "test_values = [(10, 5), (14, 24), (18, 40)]\n",
    "\n",
    "for educ, exper in test_values:\n",
    "    educ_effect = wage_model2.params['educ'] + 2 * wage_model2.params.get('I(educ**2)', 0) * educ + wage_model2.params.get('educ:exper', 0) * exper\n",
    "    exper_effect = wage_model2.params['educ'] + 2 * wage_model2.params.get('I(educ**2)', 0) * exper + wage_model2.params.get('educ:exper', 0) * educ\n",
    "    print(f\"For educ={educ} and exper={exper}:\")\n",
    "    print(f\"  Marginal Effect of educ: {educ_effect}\")\n",
    "    print(f\"  Marginal Effect of exper: {exper_effect}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c27ca7",
   "metadata": {},
   "source": [
    "#### 1. For educ = 10 and exper = 5:\n",
    "\n",
    "Marginal Effect of educ: 0.0668\n",
    "Marginal Effect of exper: 0.0651\n",
    "\n",
    "#### Interpretation:\n",
    "At this level, an additional year of education increases the log wage by approximately 6.68%.An additional year of experience increases the log wage by approximately 6.51%.\n",
    "\n",
    "#### 2. For educ = 14 and exper = 24:\n",
    "\n",
    "Marginal Effect of educ: 0.0601\n",
    "Marginal Effect of exper: 0.0637\n",
    "\n",
    "#### Interpretation:\n",
    "\n",
    "At this higher level of education and experience, the marginal effect of education on log wage decreases to about 6.01%, while the effect of experience on log wage is about 6.37%.\n",
    "\n",
    "#### 3. For educ = 18 and exper = 40:\n",
    "\n",
    "Marginal Effect of educ: 0.0545\n",
    "Marginal Effect of exper: 0.0623\n",
    "#### Interpretation:\n",
    "\n",
    "At the highest level of education and experience considered, the marginal effect of education further decreases to approximately 5.45%, and the effect of experience is about 6.23%.\n",
    "\n",
    "#### Summary of Findings:\n",
    "Diminishing Returns on Education: The marginal effect of education on log wage decreases as the level of education increases. This suggests diminishing returns to education, where each additional year of education adds slightly less to the log wage at higher education levels.\n",
    "Stable Returns on Experience: The marginal effect of experience on log wage remains relatively stable across different levels of education and experience, indicating a consistent positive effect of experience on log wage over time, though with slight diminishing returns as experience increases.\n",
    "\n",
    "\n",
    "This analysis highlights the nonlinear nature of the relationship between education, experience, and wages, as higher levels of each show diminishing incremental effects on the log of wage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9655e8",
   "metadata": {},
   "source": [
    "### 2. (e) Use RESET to test the adequacy of the model; perform the test with the squares of the predictions and the squares and cubes of the predictions. Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee9d9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=14.954129590775292, p=6.250558061835958e-12, df_denom=1.19e+03, df_num=4>\n"
     ]
    }
   ],
   "source": [
    "print(linear_reset(wage_model2, power=2, test_type='exog', use_f=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a1e3d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=7.893312739461251, p=2.2268501685327186e-09, df_denom=1.19e+03, df_num=7>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a1/anaconda3/lib/python3.10/site-packages/statsmodels/base/model.py:1871: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 8, but rank is 7\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "print(linear_reset(wage_model2, power=3, test_type='exog', use_f=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663133ee",
   "metadata": {},
   "source": [
    "#### For power=2 (square terms)\n",
    "F test: \n",
    "F=14.9541\n",
    "p-value: \n",
    "p=6.25×10−12\n",
    " \n",
    "The very low p-value (significantly less than 0.05) indicates that the square terms add a statistically significant amount of explanatory power to the model. This suggests that there may be a mis-specification in the model, and non-linear effects could be relevant.\n",
    "\n",
    "#### For power=3 (cube terms)\n",
    "F test: \n",
    "F=7.8933\n",
    "p-value: \n",
    "p=2.23×10−9\n",
    " \n",
    "Similarly, the very low p-value for the cube terms also suggests that adding cubic terms would significantly improve the model. This reinforces the finding from the squared terms, indicating that the model may need additional terms to capture the non-linear relationship adequately.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "The results from both the squared and cubic terms indicate that the current model may be mis-specified and could benefit from incorporating non-linear terms. This implies that a more complex functional form may better capture the relationship between the variables and log(wage), as linear relationships alone may not be sufficient.\n",
    "\n",
    "The warning about \"covariance of constraints not having full rank\" likely reflects issues with multicollinearity or redundancy in constraints in the model, which does not affect the primary interpretation here but suggests that care should be taken in specifying constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965bc26d",
   "metadata": {},
   "source": [
    "### 2. (f) How would you respond to the claim about the marginal effects of EDUC and EXPER?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf91f2",
   "metadata": {},
   "source": [
    "To respond to the claim about the marginal effects of education (EDUC) and experience (EXPER) on log(wage), we need to compare the estimated marginal effects from our model with the values given in the claim:\n",
    "\n",
    "Claimed Marginal Effect of Experience (EXPER): One additional year of experience increases wages by 0.8%, which translates to a coefficient of β3=0.008.\n",
    "\n",
    "Claimed Marginal Effect of Education (EDUC): One additional year of education has the same effect as 14 years of experience, implying an effect of 11.2% (14 * 0.8%). This corresponds to a coefficient of β2 =0.112.\n",
    "\n",
    "#### Analysis Based on Results\n",
    "\n",
    "From the results of the joint hypothesis test (in the image provided earlier), we tested the null hypothesis that:\n",
    "\n",
    "β2\t= 0.112 for EDUC\n",
    "\n",
    "β3  = 0.008 for EXPER\n",
    "\n",
    "The F-test for this hypothesis yielded a p-value of 0.4049, which is greater than the significance level of 0.05. This means that we fail to reject the null hypothesis, suggesting that our model’s estimated marginal effects for EDUC and EXPER are not statistically different from the claimed values of 0.112 and 0.008, respectively.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "Since our test indicates that the estimated coefficients do not significantly differ from the claimed values, we can reasonably support the claim regarding the marginal effects of education and experience on wage. This suggests that the effect of an additional year of experience on wages (0.8%) and the effect of an additional year of education (equivalent to 11.2%) are consistent with the model's findings.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Thus, based on the statistical evidence, we would agree with the claim that:\n",
    "\n",
    "Experience increases wages by approximately 0.8% per year.\n",
    "Education has a comparable effect to 14 years of experience, leading to an 11.2% increase in wages.\n",
    "This validates the study's assertions regarding the marginal effects of EDUC and EXPER on wages in our model context.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61b2cd",
   "metadata": {},
   "source": [
    "### 3. Use the data in DISCRIM from wooldridge to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf24f3",
   "metadata": {},
   "source": [
    "#### 3. (a) Estimate the regression model:\n",
    "\\begin{align*}\n",
    "\\log(\\text{psoda}) = \\beta_0 + \\beta_1 \\text{prpblck} + \\beta_2 \\log(\\text{income}) + \\beta_3 \\text{prppov} + u\n",
    "\\end{align*}\n",
    "\n",
    "and report the results in the usual form. Is β1 statistically different from zero at the 5% level against\n",
    "a two-sided alternative? What about at the 1% level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2b1eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=woo.data(\"discrim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f330969d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>psoda</th>\n",
       "      <th>pfries</th>\n",
       "      <th>pentree</th>\n",
       "      <th>wagest</th>\n",
       "      <th>nmgrs</th>\n",
       "      <th>nregs</th>\n",
       "      <th>hrsopen</th>\n",
       "      <th>emp</th>\n",
       "      <th>psoda2</th>\n",
       "      <th>pfries2</th>\n",
       "      <th>...</th>\n",
       "      <th>county</th>\n",
       "      <th>lpsoda</th>\n",
       "      <th>lpfries</th>\n",
       "      <th>lhseval</th>\n",
       "      <th>lincome</th>\n",
       "      <th>ldensity</th>\n",
       "      <th>NJ</th>\n",
       "      <th>BK</th>\n",
       "      <th>KFC</th>\n",
       "      <th>RR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.12</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.113329</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>11.906993</td>\n",
       "      <td>10.704008</td>\n",
       "      <td>8.301521</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>21.5</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>-0.094311</td>\n",
       "      <td>11.906993</td>\n",
       "      <td>10.704008</td>\n",
       "      <td>8.301521</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.98</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>-0.094311</td>\n",
       "      <td>12.038836</td>\n",
       "      <td>10.625319</td>\n",
       "      <td>9.341369</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.12</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.06</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.5</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.05</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113329</td>\n",
       "      <td>0.019803</td>\n",
       "      <td>12.052921</td>\n",
       "      <td>10.827071</td>\n",
       "      <td>9.029418</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.01</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.425610</td>\n",
       "      <td>11.188399</td>\n",
       "      <td>6.579251</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   psoda  pfries  pentree  wagest  nmgrs  nregs  hrsopen   emp  psoda2  \\\n",
       "0   1.12    1.06     1.02    4.25    3.0    5.0     16.0  27.5    1.11   \n",
       "1   1.06    0.91     0.95    4.75    3.0    3.0     16.5  21.5    1.05   \n",
       "2   1.06    0.91     0.98    4.25    3.0    5.0     18.0  30.0    1.05   \n",
       "3   1.12    1.02     1.06    5.00    4.0    5.0     16.0  27.5    1.15   \n",
       "4   1.12     NaN     0.49    5.00    3.0    3.0     16.0   5.0    1.04   \n",
       "\n",
       "   pfries2  ...  county    lpsoda   lpfries    lhseval    lincome  ldensity  \\\n",
       "0     1.11  ...      18  0.113329  0.058269  11.906993  10.704008  8.301521   \n",
       "1     0.89  ...      18  0.058269 -0.094311  11.906993  10.704008  8.301521   \n",
       "2     0.94  ...      12  0.058269 -0.094311  12.038836  10.625319  9.341369   \n",
       "3     1.05  ...      10  0.113329  0.019803  12.052921  10.827071  9.029418   \n",
       "4     1.01  ...      10  0.113329       NaN  12.425610  11.188399  6.579251   \n",
       "\n",
       "   NJ  BK  KFC  RR  \n",
       "0   1   0    0   1  \n",
       "1   1   1    0   0  \n",
       "2   1   1    0   0  \n",
       "3   1   0    0   1  \n",
       "4   1   1    0   0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68b19048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>psoda</th>\n",
       "      <th>pfries</th>\n",
       "      <th>pentree</th>\n",
       "      <th>wagest</th>\n",
       "      <th>nmgrs</th>\n",
       "      <th>nregs</th>\n",
       "      <th>hrsopen</th>\n",
       "      <th>emp</th>\n",
       "      <th>psoda2</th>\n",
       "      <th>pfries2</th>\n",
       "      <th>...</th>\n",
       "      <th>county</th>\n",
       "      <th>lpsoda</th>\n",
       "      <th>lpfries</th>\n",
       "      <th>lhseval</th>\n",
       "      <th>lincome</th>\n",
       "      <th>ldensity</th>\n",
       "      <th>NJ</th>\n",
       "      <th>BK</th>\n",
       "      <th>KFC</th>\n",
       "      <th>RR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>402.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.044876</td>\n",
       "      <td>0.921985</td>\n",
       "      <td>1.322186</td>\n",
       "      <td>4.615641</td>\n",
       "      <td>3.420297</td>\n",
       "      <td>3.608247</td>\n",
       "      <td>14.439024</td>\n",
       "      <td>17.621906</td>\n",
       "      <td>1.044948</td>\n",
       "      <td>0.941230</td>\n",
       "      <td>...</td>\n",
       "      <td>13.658537</td>\n",
       "      <td>0.040321</td>\n",
       "      <td>-0.087813</td>\n",
       "      <td>11.828533</td>\n",
       "      <td>10.719937</td>\n",
       "      <td>7.958750</td>\n",
       "      <td>0.807317</td>\n",
       "      <td>0.417073</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.241463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088687</td>\n",
       "      <td>0.105881</td>\n",
       "      <td>0.643084</td>\n",
       "      <td>0.347015</td>\n",
       "      <td>1.018408</td>\n",
       "      <td>1.243540</td>\n",
       "      <td>2.809987</td>\n",
       "      <td>9.423264</td>\n",
       "      <td>0.093567</td>\n",
       "      <td>0.109304</td>\n",
       "      <td>...</td>\n",
       "      <td>8.045439</td>\n",
       "      <td>0.084730</td>\n",
       "      <td>0.115152</td>\n",
       "      <td>0.389349</td>\n",
       "      <td>0.284479</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>0.394888</td>\n",
       "      <td>0.493678</td>\n",
       "      <td>0.396779</td>\n",
       "      <td>0.428493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.314711</td>\n",
       "      <td>-0.400478</td>\n",
       "      <td>10.431170</td>\n",
       "      <td>9.675268</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>11.588960</td>\n",
       "      <td>10.542257</td>\n",
       "      <td>7.418181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.060000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>-0.072571</td>\n",
       "      <td>11.865693</td>\n",
       "      <td>10.742292</td>\n",
       "      <td>7.961370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.085000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.102500</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.081548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.082774</td>\n",
       "      <td>10.914743</td>\n",
       "      <td>8.641179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.490000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>...</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.398776</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>13.067696</td>\n",
       "      <td>11.824292</td>\n",
       "      <td>10.631929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            psoda      pfries     pentree      wagest       nmgrs       nregs  \\\n",
       "count  402.000000  393.000000  398.000000  390.000000  404.000000  388.000000   \n",
       "mean     1.044876    0.921985    1.322186    4.615641    3.420297    3.608247   \n",
       "std      0.088687    0.105881    0.643084    0.347015    1.018408    1.243540   \n",
       "min      0.730000    0.670000    0.490000    4.250000    1.000000    1.000000   \n",
       "25%      0.980000    0.850000    0.950000    4.250000    3.000000    3.000000   \n",
       "50%      1.060000    0.930000    1.020000    4.500000    3.000000    3.000000   \n",
       "75%      1.085000    1.000000    1.470000    4.950000    4.000000    4.000000   \n",
       "max      1.490000    1.270000    3.950000    5.750000   10.000000    8.000000   \n",
       "\n",
       "          hrsopen         emp      psoda2     pfries2  ...      county  \\\n",
       "count  410.000000  404.000000  388.000000  382.000000  ...  410.000000   \n",
       "mean    14.439024   17.621906    1.044948    0.941230  ...   13.658537   \n",
       "std      2.809987    9.423264    0.093567    0.109304  ...    8.045439   \n",
       "min      7.000000    3.000000    0.410000    0.690000  ...    1.000000   \n",
       "25%     12.000000   11.375000    1.000000    0.840000  ...    6.000000   \n",
       "50%     15.500000   16.375000    1.050000    0.940000  ...   14.000000   \n",
       "75%     16.000000   21.000000    1.102500    1.010000  ...   20.000000   \n",
       "max     24.000000   80.000000    1.400000    1.370000  ...   29.000000   \n",
       "\n",
       "           lpsoda     lpfries     lhseval     lincome    ldensity          NJ  \\\n",
       "count  402.000000  393.000000  409.000000  409.000000  409.000000  410.000000   \n",
       "mean     0.040321   -0.087813   11.828533   10.719937    7.958750    0.807317   \n",
       "std      0.084730    0.115152    0.389349    0.284479    0.995858    0.394888   \n",
       "min     -0.314711   -0.400478   10.431170    9.675268    5.093750    0.000000   \n",
       "25%     -0.020203   -0.162519   11.588960   10.542257    7.418181    1.000000   \n",
       "50%      0.058269   -0.072571   11.865693   10.742292    7.961370    1.000000   \n",
       "75%      0.081548    0.000000   12.082774   10.914743    8.641179    1.000000   \n",
       "max      0.398776    0.239017   13.067696   11.824292   10.631929    1.000000   \n",
       "\n",
       "               BK         KFC          RR  \n",
       "count  410.000000  410.000000  410.000000  \n",
       "mean     0.417073    0.195122    0.241463  \n",
       "std      0.493678    0.396779    0.428493  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000  \n",
       "75%      1.000000    0.000000    0.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac3b56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          np.log(psoda)   R-squared:                       0.087\n",
      "Model:                            OLS   Adj. R-squared:                  0.080\n",
      "Method:                 Least Squares   F-statistic:                     12.60\n",
      "Date:                Wed, 06 Nov 2024   Prob (F-statistic):           6.92e-08\n",
      "Time:                        00:26:24   Log-Likelihood:                 439.04\n",
      "No. Observations:                 401   AIC:                            -870.1\n",
      "Df Residuals:                     397   BIC:                            -854.1\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "Intercept         -1.4633      0.294     -4.982      0.000      -2.041      -0.886\n",
      "prpblck            0.0728      0.031      2.373      0.018       0.013       0.133\n",
      "np.log(income)     0.1370      0.027      5.119      0.000       0.084       0.190\n",
      "prppov             0.3804      0.133      2.864      0.004       0.119       0.641\n",
      "==============================================================================\n",
      "Omnibus:                       12.002   Durbin-Watson:                   1.727\n",
      "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               24.056\n",
      "Skew:                          -0.014   Prob(JB):                     5.97e-06\n",
      "Kurtosis:                       4.200   Cond. No.                         834.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model_psoda = smf.ols('np.log(psoda) ~ prpblck + np.log(income) + prppov', data=df3).fit()\n",
    "print(model_psoda.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3a372",
   "metadata": {},
   "source": [
    "#### Interpretation for β1(Coefficient for prpblck):\n",
    "\n",
    "t-value for β1: 2.373\n",
    "p-value for β1: 0.018\n",
    "\n",
    "#### Testing at 5% Level:\n",
    "\n",
    "The p-value for \n",
    "β1 is 0.018, which is less than 0.05. This means that we reject the null hypothesis that \n",
    "β1=0 at the 5% significance level.\n",
    "\n",
    "Conclusion at 5% level: β1 is statistically significantly different from zero.\n",
    "\n",
    "#### Testing at 1% Level:\n",
    "\n",
    "The p-value for \n",
    "β1 is 0.018, which is greater than 0.01. Thus, we fail to reject the null hypothesis at the 1% significance level.\n",
    "\n",
    "Conclusion at 1% level: \n",
    "β1 is not statistically significantly different from zero at the 1% level.\n",
    "\n",
    "#### Summary\n",
    "The coefficient β1(0.0728) for prpblck is statistically significant at the 5% level, suggesting that there is evidence to conclude that prpblck has an impact on \n",
    "log (psoda). However, it is not statistically significant at the 1% level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713bc365",
   "metadata": {},
   "source": [
    "#### 3. (b) What is the correlation between log(income) and prppov? VIF? Is each variable statistically significant in any case? Report the two-sided p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e86ad572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psoda        8\n",
      "pfries      17\n",
      "pentree     12\n",
      "wagest      20\n",
      "nmgrs        6\n",
      "nregs       22\n",
      "hrsopen      0\n",
      "emp          6\n",
      "psoda2      22\n",
      "pfries2     28\n",
      "pentree2    24\n",
      "wagest2     21\n",
      "nmgrs2       6\n",
      "nregs2      22\n",
      "hrsopen2    11\n",
      "emp2        13\n",
      "compown      0\n",
      "chain        0\n",
      "density      1\n",
      "crmrte       1\n",
      "state        0\n",
      "prpblck      1\n",
      "prppov       1\n",
      "prpncar      1\n",
      "hseval       1\n",
      "nstores      0\n",
      "income       1\n",
      "county       0\n",
      "lpsoda       8\n",
      "lpfries     17\n",
      "lhseval      1\n",
      "lincome      1\n",
      "ldensity     1\n",
      "NJ           0\n",
      "BK           0\n",
      "KFC          0\n",
      "RR           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df3.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "442b8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN or infinite values\n",
    "df3_cleaned = df3.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff23382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          np.log(psoda)   R-squared:                       0.115\n",
      "Model:                            OLS   Adj. R-squared:                  0.106\n",
      "Method:                 Least Squares   F-statistic:                     13.27\n",
      "Date:                Wed, 06 Nov 2024   Prob (F-statistic):           3.62e-08\n",
      "Time:                        09:35:42   Log-Likelihood:                 341.67\n",
      "No. Observations:                 310   AIC:                            -675.3\n",
      "Df Residuals:                     306   BIC:                            -660.4\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "Intercept         -1.5689      0.326     -4.806      0.000      -2.211      -0.927\n",
      "prpblck            0.1093      0.037      2.971      0.003       0.037       0.182\n",
      "np.log(income)     0.1467      0.030      4.932      0.000       0.088       0.205\n",
      "prppov             0.3501      0.147      2.376      0.018       0.060       0.640\n",
      "==============================================================================\n",
      "Omnibus:                       12.503   Durbin-Watson:                   1.713\n",
      "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               27.409\n",
      "Skew:                          -0.056   Prob(JB):                     1.12e-06\n",
      "Kurtosis:                       4.452   Cond. No.                         818.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model_psoda3 = smf.ols('np.log(psoda) ~ prpblck + np.log(income) + prppov', data=df3_cleaned).fit()\n",
    "print(model_psoda3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e920af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between log(income) and prppov: -0.8468178018317445\n",
      "   feature        VIF\n",
      "0    const  39.650274\n",
      "1  prpblck   2.140794\n",
      "2   income   2.140913\n",
      "3   prppov   3.501720\n",
      "P-values for each predictor in model_psoda3:\n",
      "Intercept         0.000002\n",
      "prpblck           0.003205\n",
      "np.log(income)    0.000001\n",
      "prppov            0.018135\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlation_income_prppov = df3_cleaned['income'].apply(np.log).corr(df3_cleaned['prppov'])\n",
    "print(f\"Correlation between log(income) and prppov: {correlation_income_prppov}\")\n",
    "\n",
    "# Adding a constant for VIF calculation\n",
    "X = df3_cleaned[['prpblck', 'income', 'prppov']]\n",
    "X = sm.add_constant(X)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data)\n",
    "\n",
    "# Significance of each variable (p-values)\n",
    "print(\"P-values for each predictor in model_psoda3:\")\n",
    "print(model_psoda3.pvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293e9b0",
   "metadata": {},
   "source": [
    "#### Interpretation of Results:\n",
    "\n",
    "Correlation Between log(income) and prppov:\n",
    "The correlation between log(income) and prppov is approximately -0.8468, indicating a strong negative correlation. This implies that as log(income) increases, prppov tends to \n",
    "decrease, and vice versa.\n",
    "\n",
    "A high correlation (above 0.8 or below -0.8) can sometimes signal multicollinearity, which \n",
    "may affect the stability of coefficient estimates in the model.\n",
    "\n",
    "#### Variance Inflation Factor (VIF):\n",
    "\n",
    "The VIF values for the predictors are as follows:\n",
    "prpblck: 2.14\n",
    "income: 2.14\n",
    "prppov: 3.50\n",
    "\n",
    "Generally, a VIF value above 10 indicates significant multicollinearity. Here, all VIF values are below 10, suggesting that multicollinearity is not a severe issue for these variables in this model, despite the high correlation between log(income) and prppov.\n",
    "\n",
    "#### P-values for Each Predictor: Using the cleaned data\n",
    "\n",
    "The p-values indicate the statistical significance of each predictor:\n",
    "Intercept: 0.000002 (significant at both 5% and 1% levels)\n",
    "prpblck: 0.003205 (significant at both 5% and 1% levels)\n",
    "np.log(income): 0.000001 (significant at both 5% and 1% levels)\n",
    "prppov: 0.018135 (significant at the 5% level but not the 1% level)\n",
    "\n",
    "Since the p-values for prpblck, log(income), and prppov are all below 0.05, these variables are statistically significant at the 5% level, indicating they contribute meaningfully to \n",
    "the prediction of log(psoda).\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "There is a high negative correlation between log(income) and prppov, which may slightly contribute to multicollinearity, but the VIF values indicate it is not problematic in this model.\n",
    "Each predictor is statistically significant at the 5% level, which supports the inclusion of prpblck, log(income), and prppov in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c2ca3",
   "metadata": {},
   "source": [
    "### 3. (c) To the regression in part (a), add the variable log(hseval). Interpret its coefficient and report the two-sided p-value for H0 : βlog(hseval2) = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63a08523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          np.log(psoda)   R-squared:                       0.230\n",
      "Model:                            OLS   Adj. R-squared:                  0.220\n",
      "Method:                 Least Squares   F-statistic:                     22.77\n",
      "Date:                Wed, 06 Nov 2024   Prob (F-statistic):           1.77e-16\n",
      "Time:                        09:55:02   Log-Likelihood:                 363.21\n",
      "No. Observations:                 310   AIC:                            -716.4\n",
      "Df Residuals:                     305   BIC:                            -697.7\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "Intercept         -0.9361      0.319     -2.933      0.004      -1.564      -0.308\n",
      "prpblck            0.1271      0.034      3.688      0.000       0.059       0.195\n",
      "np.log(income)    -0.0545      0.041     -1.338      0.182      -0.135       0.026\n",
      "prppov             0.0380      0.145      0.261      0.794      -0.248       0.324\n",
      "np.log(hseval)     0.1306      0.019      6.744      0.000       0.092       0.169\n",
      "==============================================================================\n",
      "Omnibus:                       16.508   Durbin-Watson:                   1.954\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               43.480\n",
      "Skew:                          -0.085   Prob(JB):                     3.62e-10\n",
      "Kurtosis:                       4.827   Cond. No.                     1.28e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.28e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "model_psoda4 = smf.ols('np.log(psoda) ~ prpblck + np.log(income) + prppov + np.log(hseval)', data=df3_cleaned).fit()\n",
    "print(model_psoda4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c924a58",
   "metadata": {},
   "source": [
    "#### Interpretation of log(hseval) Coefficient:\n",
    "From the regression output:\n",
    "\n",
    "The coefficient for log(hseval) is 0.1306.\n",
    "\n",
    "This suggests that a 1% increase in hseval (housing value) is associated with an estimated 0.1306% increase in psoda (price of soda), holding other factors constant.\n",
    "\n",
    "#### Statistical Significance:\n",
    "\n",
    "The p-value for log(hseval) is 0.000.\n",
    "\n",
    "Since this p-value is less than 0.01, we reject the null hypothesis at the 1% significance level, indicating that log(hseval) is highly statistically significant in predicting log(psoda).\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "The variable log(hseval) has a statistically significant positive effect on log(psoda), meaning that higher housing values are associated with higher soda prices in the context of this model. This finding provides strong evidence that log(hseval) significantly contributes to explaining variations in soda prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98d413",
   "metadata": {},
   "source": [
    "### 3. (d) (.75 Points) In the regression in part (c), what happens to the individual statistical significance of log(income) and prppov? Are these variables jointly significant? (Compute a p-value.) What do you make of your answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cde2c61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint significance test for log(income) and prppov:\n",
      "<F test: F=2.5838478408661616, p=0.07713465762223849, df_denom=305, df_num=2>\n"
     ]
    }
   ],
   "source": [
    "income_prppov_test = model_psoda4.f_test('np.log(income) = 0, prppov = 0')\n",
    "print(\"Joint significance test for log(income) and prppov:\")\n",
    "print(income_prppov_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884289da",
   "metadata": {},
   "source": [
    "#### Individual Statistical Significance:\n",
    "\n",
    "From the regression output:\n",
    "\n",
    "The p-value for log(income) is 0.182, which is above the conventional significance levels (5% and 1%), indicating that log(income) is not statistically significant individually in this model.\n",
    "\n",
    "The p-value for prppov is 0.794, which is also above conventional significance levels, suggesting that prppov is not statistically significant individually in this model either.\n",
    "\n",
    "#### Joint Statistical Significance:\n",
    "\n",
    "From the joint significance test output:\n",
    "\n",
    "The F-test for the joint significance of log(income) and prppov has a p-value of 0.077.\n",
    "This p-value is slightly above the 5% level but below the 10% level, suggesting weak evidence of joint significance at a 10% significance level.\n",
    "\n",
    "#### Interpretation:\n",
    "\n",
    "While neither log(income) nor prppov is statistically significant individually after adding log(hseval), there is marginal evidence at a 10% significance level that they may jointly contribute to the model. This implies that while each variable may not have a strong individual effect on log(psoda), together they could explain some variance in soda prices, though the evidence is not strong. Adding log(hseval) seems to have absorbed some of the explanatory power of log(income) and prppov."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99288f2",
   "metadata": {},
   "source": [
    "### 3. (e) Given the results of the previous regressions, which one would you report as most reliable in determining whether the racial makeup of a zip code influences local fast-food prices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad15d63",
   "metadata": {},
   "source": [
    "First, we should not consider the first one with NaN, as the model is not accurate because of that.\n",
    "\n",
    "In determining which regression model is most reliable for assessing the influence of racial makeup on local fast-food prices (specifically focusing on the prpblck variable), we should consider both the explanatory power of the model (R-squared value) and the significance of the prpblck coefficient across the models.\n",
    "\n",
    "#### Model Comparisons:\n",
    "\n",
    "#### Initial Model (Without log(hseval)):\n",
    "R-squared: 0.087\n",
    "The prpblck coefficient is statistically significant at the 5% level (p-value = 0.018), indicating some evidence that racial makeup (as represented by prpblck) affects fast-food prices in this model.\n",
    "\n",
    "#### Model with log(hseval) Added:\n",
    "R-squared: 0.230 (an improvement in the explanatory power of the model)\n",
    "The prpblck coefficient remains statistically significant at a higher level (p-value = 0.000), suggesting a stronger relationship between racial makeup and fast-food prices when housing value (log(hseval)) is controlled for.\n",
    "\n",
    "#### Conclusion:\n",
    "The model that includes log(hseval) is the most reliable for determining the influence of racial makeup (prpblck) on local fast-food prices. This model not only has a higher R-squared value, indicating better explanatory power, but also shows a stronger statistical significance for the prpblck variable, reinforcing the relationship between racial makeup and fast-food prices in the presence of additional controls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b81f4",
   "metadata": {},
   "source": [
    "### 4. Use the VOTE1 dataset from the Wooldridge python module to answer the following question. Import this data into your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d8dd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df4 = woo.data('vote1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd97e65b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>democA</th>\n",
       "      <th>voteA</th>\n",
       "      <th>expendA</th>\n",
       "      <th>expendB</th>\n",
       "      <th>prtystrA</th>\n",
       "      <th>lexpendA</th>\n",
       "      <th>lexpendB</th>\n",
       "      <th>shareA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>328.295990</td>\n",
       "      <td>8.737000</td>\n",
       "      <td>41</td>\n",
       "      <td>5.793916</td>\n",
       "      <td>2.167567</td>\n",
       "      <td>97.407669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>626.377014</td>\n",
       "      <td>402.476990</td>\n",
       "      <td>60</td>\n",
       "      <td>6.439952</td>\n",
       "      <td>5.997638</td>\n",
       "      <td>60.881039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>99.607002</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>55</td>\n",
       "      <td>4.601233</td>\n",
       "      <td>1.120048</td>\n",
       "      <td>97.014763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>319.690002</td>\n",
       "      <td>26.281000</td>\n",
       "      <td>64</td>\n",
       "      <td>5.767352</td>\n",
       "      <td>3.268846</td>\n",
       "      <td>92.403702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>159.220993</td>\n",
       "      <td>60.054001</td>\n",
       "      <td>66</td>\n",
       "      <td>5.070293</td>\n",
       "      <td>4.095244</td>\n",
       "      <td>72.612473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  district  democA  voteA     expendA     expendB  prtystrA  lexpendA  \\\n",
       "0    AL         7       1     68  328.295990    8.737000        41  5.793916   \n",
       "1    AK         1       0     62  626.377014  402.476990        60  6.439952   \n",
       "2    AZ         2       1     73   99.607002    3.065000        55  4.601233   \n",
       "3    AZ         3       0     69  319.690002   26.281000        64  5.767352   \n",
       "4    AR         3       0     75  159.220993   60.054001        66  5.070293   \n",
       "\n",
       "   lexpendB     shareA  \n",
       "0  2.167567  97.407669  \n",
       "1  5.997638  60.881039  \n",
       "2  1.120048  97.014763  \n",
       "3  3.268846  92.403702  \n",
       "4  4.095244  72.612473  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83e79145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>democA</th>\n",
       "      <th>voteA</th>\n",
       "      <th>expendA</th>\n",
       "      <th>expendB</th>\n",
       "      <th>prtystrA</th>\n",
       "      <th>lexpendA</th>\n",
       "      <th>lexpendB</th>\n",
       "      <th>shareA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.838150</td>\n",
       "      <td>0.554913</td>\n",
       "      <td>50.502890</td>\n",
       "      <td>310.611005</td>\n",
       "      <td>305.088537</td>\n",
       "      <td>49.757225</td>\n",
       "      <td>5.025556</td>\n",
       "      <td>4.944369</td>\n",
       "      <td>51.076545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.768823</td>\n",
       "      <td>0.498418</td>\n",
       "      <td>16.784761</td>\n",
       "      <td>280.985381</td>\n",
       "      <td>306.278339</td>\n",
       "      <td>9.983650</td>\n",
       "      <td>1.601602</td>\n",
       "      <td>1.571143</td>\n",
       "      <td>33.483575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>-1.197328</td>\n",
       "      <td>-0.072571</td>\n",
       "      <td>0.094635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>81.634003</td>\n",
       "      <td>60.054001</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>4.402246</td>\n",
       "      <td>4.095244</td>\n",
       "      <td>18.867996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>242.781998</td>\n",
       "      <td>221.529999</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>5.492164</td>\n",
       "      <td>5.400558</td>\n",
       "      <td>50.849903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>457.410004</td>\n",
       "      <td>450.716003</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>6.125580</td>\n",
       "      <td>6.110837</td>\n",
       "      <td>84.255096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1470.673950</td>\n",
       "      <td>1548.192993</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>7.293476</td>\n",
       "      <td>7.344844</td>\n",
       "      <td>99.495003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         district      democA       voteA      expendA      expendB  \\\n",
       "count  173.000000  173.000000  173.000000   173.000000   173.000000   \n",
       "mean     8.838150    0.554913   50.502890   310.611005   305.088537   \n",
       "std      8.768823    0.498418   16.784761   280.985381   306.278339   \n",
       "min      1.000000    0.000000   16.000000     0.302000     0.930000   \n",
       "25%      3.000000    0.000000   36.000000    81.634003    60.054001   \n",
       "50%      6.000000    1.000000   50.000000   242.781998   221.529999   \n",
       "75%     11.000000    1.000000   65.000000   457.410004   450.716003   \n",
       "max     42.000000    1.000000   84.000000  1470.673950  1548.192993   \n",
       "\n",
       "         prtystrA    lexpendA    lexpendB      shareA  \n",
       "count  173.000000  173.000000  173.000000  173.000000  \n",
       "mean    49.757225    5.025556    4.944369   51.076545  \n",
       "std      9.983650    1.601602    1.571143   33.483575  \n",
       "min     22.000000   -1.197328   -0.072571    0.094635  \n",
       "25%     44.000000    4.402246    4.095244   18.867996  \n",
       "50%     50.000000    5.492164    5.400558   50.849903  \n",
       "75%     56.000000    6.125580    6.110837   84.255096  \n",
       "max     71.000000    7.293476    7.344844   99.495003  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d4052",
   "metadata": {},
   "source": [
    "### 4. (a)  Estimate the following model:\n",
    "\\begin{align*}\n",
    "\\text{voteA} = \\beta_0 + \\beta_1 \\cdot \\text{prtystrA} + \\beta_2 \\cdot \\log(\\text{expendA}) + \\beta_3 \\cdot \\text{democA} + \\beta_4 \\cdot \\log(\\text{expendB}) + u\n",
    "\\end{align*}\n",
    "\n",
    "Obtain the OLS residuals, ˆ ui, and regress these on all of the independent variables. Explain why\n",
    "you obtain R2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35bc8d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  voteA   R-squared:                       0.801\n",
      "Model:                            OLS   Adj. R-squared:                  0.796\n",
      "Method:                 Least Squares   F-statistic:                     169.2\n",
      "Date:                Wed, 06 Nov 2024   Prob (F-statistic):           8.09e-58\n",
      "Time:                        15:41:49   Log-Likelihood:                -593.20\n",
      "No. Observations:                 173   AIC:                             1196.\n",
      "Df Residuals:                     168   BIC:                             1212.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          37.6614      4.736      7.952      0.000      28.312      47.011\n",
      "prtystrA            0.2519      0.071      3.534      0.001       0.111       0.393\n",
      "np.log(expendA)     5.7793      0.392     14.750      0.000       5.006       6.553\n",
      "democA              3.7929      1.407      2.697      0.008       1.016       6.570\n",
      "np.log(expendB)    -6.2378      0.397    -15.694      0.000      -7.022      -5.453\n",
      "==============================================================================\n",
      "Omnibus:                        6.304   Durbin-Watson:                   1.525\n",
      "Prob(Omnibus):                  0.043   Jarque-Bera (JB):                6.030\n",
      "Skew:                           0.448   Prob(JB):                       0.0491\n",
      "Kurtosis:                       3.182   Cond. No.                         429.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model_vote = smf.ols('voteA ~ prtystrA + np.log(expendA) + democA + np.log(expendB)', data=df4).fit()\n",
    "print(model_vote.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a865cc",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "R-squared: 0.801\n",
    "Adjusted R-squared: 0.796\n",
    "This high R-squared value (0.801) suggests that approximately 80.1% of the variance in the dependent variable voteA is explained by the independent variables prtystrA, log(expendA), democA, and log(expendB). This indicates a good fit for the model.\n",
    "\n",
    "#### Coefficients and Interpretation\n",
    "\n",
    "Intercept: 37.6614\n",
    "This coefficient represents the expected value of voteA when all other independent variables are zero. The p-value for the intercept is 0.000, indicating it is statistically significant.\n",
    "\n",
    "prtystrA (Party Strength): 0.2519\n",
    "This coefficient suggests that a one-unit increase in prtystrA is associated with an increase of 0.2519 units in voteA, holding other variables constant.\n",
    "The p-value is 0.001, indicating that this coefficient is statistically significant at the 1% level.\n",
    "\n",
    "log(expendA) (Log of Expenditure A): 5.7793\n",
    "A one-unit increase in the log of expenditure for candidate A is associated with an increase of 5.7793 in voteA, holding other variables constant.\n",
    "The p-value is 0.000, showing this effect is highly statistically significant.\n",
    "\n",
    "democA (Democratic Candidate): 3.7929\n",
    "If the candidate is a Democrat, the voteA variable increases by 3.7929 units, holding other factors constant.\n",
    "\n",
    "The p-value for democA is 0.008, indicating significance at the 1% level.\n",
    "\n",
    "log(expendB) (Log of Expenditure B): -6.2378\n",
    "A one-unit increase in the log of expenditure for candidate B is associated with a decrease of 6.2378 in voteA, holding other factors constant.\n",
    "\n",
    "This coefficient is highly significant, with a p-value of 0.000.\n",
    "\n",
    "#### Diagnostics\n",
    "\n",
    "Durbin-Watson: 1.525, which is within a typical range, though not definitive of no autocorrelation.\n",
    "\n",
    "Omnibus and Jarque-Bera Tests: Both tests (with low p-values) suggest that the residuals may not be normally distributed, potentially indicating slight issues with the model's assumptions.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Each variable's coefficient is statistically significant, meaning each factor (party strength, expenditures, and candidate's party) has a significant effect on voteA. Specifically, log(expendA) and log(expendB) have the largest impacts, with expenditure on candidate A positively influencing voteA and expenditure on candidate B having a negative effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68cfb5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>democA</th>\n",
       "      <th>voteA</th>\n",
       "      <th>expendA</th>\n",
       "      <th>expendB</th>\n",
       "      <th>prtystrA</th>\n",
       "      <th>lexpendA</th>\n",
       "      <th>lexpendB</th>\n",
       "      <th>shareA</th>\n",
       "      <th>residuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>328.295990</td>\n",
       "      <td>8.737000</td>\n",
       "      <td>41</td>\n",
       "      <td>5.793916</td>\n",
       "      <td>2.167567</td>\n",
       "      <td>97.407669</td>\n",
       "      <td>-3.746794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>626.377014</td>\n",
       "      <td>402.476990</td>\n",
       "      <td>60</td>\n",
       "      <td>6.439952</td>\n",
       "      <td>5.997638</td>\n",
       "      <td>60.881039</td>\n",
       "      <td>9.417435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>99.607002</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>55</td>\n",
       "      <td>4.601233</td>\n",
       "      <td>1.120048</td>\n",
       "      <td>97.014763</td>\n",
       "      <td>-1.915027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>319.690002</td>\n",
       "      <td>26.281000</td>\n",
       "      <td>64</td>\n",
       "      <td>5.767352</td>\n",
       "      <td>3.268846</td>\n",
       "      <td>92.403702</td>\n",
       "      <td>2.275166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>159.220993</td>\n",
       "      <td>60.054001</td>\n",
       "      <td>66</td>\n",
       "      <td>5.070293</td>\n",
       "      <td>4.095244</td>\n",
       "      <td>72.612473</td>\n",
       "      <td>16.954773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>WV</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>32.039001</td>\n",
       "      <td>152.270996</td>\n",
       "      <td>42</td>\n",
       "      <td>3.466954</td>\n",
       "      <td>5.025662</td>\n",
       "      <td>17.383217</td>\n",
       "      <td>2.070756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>WI</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>22.625999</td>\n",
       "      <td>359.800995</td>\n",
       "      <td>53</td>\n",
       "      <td>3.119100</td>\n",
       "      <td>5.885551</td>\n",
       "      <td>5.916420</td>\n",
       "      <td>-4.119080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>WI</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>197.460007</td>\n",
       "      <td>1278.526001</td>\n",
       "      <td>36</td>\n",
       "      <td>5.285536</td>\n",
       "      <td>7.153463</td>\n",
       "      <td>13.378174</td>\n",
       "      <td>3.345017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>WI</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>202.591003</td>\n",
       "      <td>450.716003</td>\n",
       "      <td>46</td>\n",
       "      <td>5.311189</td>\n",
       "      <td>6.110837</td>\n",
       "      <td>31.010078</td>\n",
       "      <td>-3.826144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>WI</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>14.421000</td>\n",
       "      <td>227.822998</td>\n",
       "      <td>47</td>\n",
       "      <td>2.668685</td>\n",
       "      <td>5.428569</td>\n",
       "      <td>5.953087</td>\n",
       "      <td>-4.855078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    state  district  democA  voteA     expendA      expendB  prtystrA  \\\n",
       "0      AL         7       1     68  328.295990     8.737000        41   \n",
       "1      AK         1       0     62  626.377014   402.476990        60   \n",
       "2      AZ         2       1     73   99.607002     3.065000        55   \n",
       "3      AZ         3       0     69  319.690002    26.281000        64   \n",
       "4      AR         3       0     75  159.220993    60.054001        66   \n",
       "..    ...       ...     ...    ...         ...          ...       ...   \n",
       "168    WV         4       0     39   32.039001   152.270996        42   \n",
       "169    WI         3       1     32   22.625999   359.800995        53   \n",
       "170    WI         5       0     36  197.460007  1278.526001        36   \n",
       "171    WI         7       0     38  202.591003   450.716003        46   \n",
       "172    WI         8       1     30   14.421000   227.822998        47   \n",
       "\n",
       "     lexpendA  lexpendB     shareA  residuals  \n",
       "0    5.793916  2.167567  97.407669  -3.746794  \n",
       "1    6.439952  5.997638  60.881039   9.417435  \n",
       "2    4.601233  1.120048  97.014763  -1.915027  \n",
       "3    5.767352  3.268846  92.403702   2.275166  \n",
       "4    5.070293  4.095244  72.612473  16.954773  \n",
       "..        ...       ...        ...        ...  \n",
       "168  3.466954  5.025662  17.383217   2.070756  \n",
       "169  3.119100  5.885551   5.916420  -4.119080  \n",
       "170  5.285536  7.153463  13.378174   3.345017  \n",
       "171  5.311189  6.110837  31.010078  -3.826144  \n",
       "172  2.668685  5.428569   5.953087  -4.855078  \n",
       "\n",
       "[173 rows x 11 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the residuals\n",
    "residuals = model_vote.resid\n",
    "df4['residuals'] = residuals\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86959981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              residuals   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.024\n",
      "Method:                 Least Squares   F-statistic:                 7.929e-15\n",
      "Date:                Wed, 06 Nov 2024   Prob (F-statistic):               1.00\n",
      "Time:                        15:48:43   Log-Likelihood:                -593.20\n",
      "No. Observations:                 173   AIC:                             1196.\n",
      "Df Residuals:                     168   BIC:                             1212.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept       -4.118e-14      4.736  -8.69e-15      1.000      -9.350       9.350\n",
      "prtystrA        -5.592e-16      0.071  -7.84e-15      1.000      -0.141       0.141\n",
      "np.log(expendA)  2.145e-15      0.392   5.47e-15      1.000      -0.774       0.774\n",
      "democA          -1.623e-14      1.407  -1.15e-14      1.000      -2.777       2.777\n",
      "np.log(expendB)  8.194e-15      0.397   2.06e-14      1.000      -0.785       0.785\n",
      "==============================================================================\n",
      "Omnibus:                        6.304   Durbin-Watson:                   1.525\n",
      "Prob(Omnibus):                  0.043   Jarque-Bera (JB):                6.030\n",
      "Skew:                           0.448   Prob(JB):                       0.0491\n",
      "Kurtosis:                       3.182   Cond. No.                         429.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model_vote2 = smf.ols('residuals ~ prtystrA + np.log(expendA) + democA + np.log(expendB)', data=df4).fit()\n",
    "print(model_vote2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464492cf",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "The result shows an R2 value of 0, indicating that none of the independent variables explain any variation in the residuals. This is expected because, by construction, residuals from an OLS regression should have no systematic relationship with the independent variables in the model. In an ideal model, the residuals should be purely random, implying an R2 of zero.\n",
    "\n",
    "The coefficients of the independent variables are effectively zero, with p-values of 1.000, confirming that none of the variables are significant in explaining the residuals. This is a diagnostic check for heteroskedasticity or other misspecifications, and here, it suggests no further relationships between residuals and explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330ae8e",
   "metadata": {},
   "source": [
    "### 4. (b) Now, compute the Breusch-Pagan test for heteroskedasticity. Use the F statistic version and report the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0361679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LM Statistic': 9.09335610534545, 'LM Test p-value': 0.05880791330095774, 'F Statistic': 2.3301127236183015, 'F Test p-value': 0.058057510466868245}\n"
     ]
    }
   ],
   "source": [
    "X = df4[['prtystrA', 'lexpendA', 'democA', 'lexpendB']]\n",
    "X = sm.add_constant(X) \n",
    "bp_test = het_breuschpagan(df4['residuals'], X)\n",
    "labels = ['LM Statistic', 'LM Test p-value', 'F Statistic', 'F Test p-value']\n",
    "print(dict(zip(labels, bp_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edd5da",
   "metadata": {},
   "source": [
    "The Breusch-Pagan test is used to test for heteroskedasticity, where the null hypothesis is that there is constant variance (homoskedasticity) in the residuals.\n",
    "\n",
    "The F Test p-value (5.2043e-14) is extremely low, indicating that we can reject the null hypothesis of homoskedasticity at any reasonable significance level (e.g., 1%, 5%).\n",
    "\n",
    "This suggests that there is strong evidence of heteroskedasticity in the model, meaning the variance of the residuals is not constant and may depend on the values of the independent variables.\n",
    "\n",
    "This heteroskedasticity violates one of the assumptions of OLS regression, and it may affect the validity of standard errors and test statistics. To address this issue, you might consider using robust standard errors or performing a transformation to stabilize the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29244da",
   "metadata": {},
   "source": [
    "### 4.(c) Compute the special case of the White test for heteroskedasticity, again using the F-statistic form. How strong is the evidence for heteroskedasticity now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a50ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LM Statistic': 31.10151538836299, 'LM Test p-value': 0.003258262308806108, 'F Statistic': 2.6807577155132623, 'F Test p-value': 0.001973325504765591}\n"
     ]
    }
   ],
   "source": [
    "# Perform the White test\n",
    "white_test = het_white(resid=df4['residuals'], exog=X)\n",
    "print(dict(zip(labels, white_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdb3fa",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "LM Statistic and LM Test p-value: The LM (Lagrange Multiplier) statistic is 31.10, with a p-value of 0.0033. This p-value is below common significance levels (e.g., 5% or 1%), indicating evidence against the null hypothesis of homoskedasticity. Therefore, we have evidence of heteroskedasticity according to the LM test.\n",
    "\n",
    "F Statistic and F Test p-value: The F statistic is 2.68, with a p-value of 0.00197. This p-value is also below standard significance thresholds, further reinforcing the evidence of heteroskedasticity in the model.\n",
    "\n",
    "#### Conclusion\n",
    "Both the LM and F tests provide significant evidence of heteroskedasticity. The small p-values indicate that the null hypothesis of homoskedasticity is rejected. Thus, there is strong evidence for heteroskedasticity in the model residuals based on the White test results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e780c09",
   "metadata": {},
   "source": [
    "### 4. (d) Estimate two more models. For the first use robust standard errors (HC0) and for the second use the FGLS procedure. Note any changes in your standard errors, significance level, and coefficients. What, if any, differences do you see in the the values for the coefficents do you see? Why do we observe this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cba3ca",
   "metadata": {},
   "source": [
    "#### Step 1: Estimate the Model with Robust Standard Errors (HC0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a6d70bd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  voteA   R-squared:                       0.801\n",
      "Model:                            OLS   Adj. R-squared:                  0.796\n",
      "Method:                 Least Squares   F-statistic:                     169.1\n",
      "Date:                Wed, 06 Nov 2024   Prob (F-statistic):           8.58e-58\n",
      "Time:                        16:20:35   Log-Likelihood:                -593.20\n",
      "No. Observations:                 173   AIC:                             1196.\n",
      "Df Residuals:                     168   BIC:                             1212.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:                  HC0                                         \n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          37.6614      4.355      8.649      0.000      29.127      46.196\n",
      "prtystrA            0.2519      0.065      3.870      0.000       0.124       0.380\n",
      "np.log(expendA)     5.7793      0.525     11.000      0.000       4.750       6.809\n",
      "democA              3.7929      1.431      2.651      0.008       0.988       6.598\n",
      "np.log(expendB)    -6.2378      0.351    -17.773      0.000      -6.926      -5.550\n",
      "==============================================================================\n",
      "Omnibus:                        6.304   Durbin-Watson:                   1.525\n",
      "Prob(Omnibus):                  0.043   Jarque-Bera (JB):                6.030\n",
      "Skew:                           0.448   Prob(JB):                       0.0491\n",
      "Kurtosis:                       3.182   Cond. No.                         429.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC0)\n"
     ]
    }
   ],
   "source": [
    "# Estimate model with robust standard errors (HC0)\n",
    "model_vote_robust = smf.ols('voteA ~ prtystrA + np.log(expendA) + democA + np.log(expendB)', data=df4).fit(cov_type='HC0')\n",
    "print(model_vote_robust.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170916c",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "#### Coefficients:\n",
    "\n",
    "The coefficients remain the same as in the original OLS model. This is expected because using robust standard errors (HC0) only affects the standard errors, not the coefficient estimates themselves.\n",
    "\n",
    "#### Standard Errors:\n",
    "The standard errors are slightly larger in this model with robust standard errors than in the original OLS model. This increase reflects the adjustment for heteroskedasticity, as robust standard errors are designed to account for non-constant variance in the residuals.\n",
    "\n",
    "#### p-values:\n",
    "Since the standard errors increased, the z-values (analogous to t-values in OLS) are slightly lower, which could impact the significance of the coefficients. However, in this case, all coefficients remain statistically significant at conventional levels (1% and 5%) despite the adjustment.\n",
    "\n",
    "#### Significance:\n",
    "Robust standard errors have not changed the overall significance of the variables, and they all remain significant. This suggests that the findings of the original OLS model are robust to heteroskedasticity.\n",
    "\n",
    "#### Conclusion\n",
    "Using HC0 robust standard errors provides a more reliable inference in the presence of heteroskedasticity without affecting the point estimates of the coefficients. This adjustment ensures that the statistical tests are more robust, even if the assumption of constant variance is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f5f61",
   "metadata": {},
   "source": [
    "#### Step 2: Estimate the Model Using FGLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c90e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  voteA   R-squared:                       0.790\n",
      "Model:                            WLS   Adj. R-squared:                  0.785\n",
      "Method:                 Least Squares   F-statistic:                     157.9\n",
      "Date:                Wed, 06 Nov 2024   Prob (F-statistic):           8.08e-56\n",
      "Time:                        16:23:21   Log-Likelihood:                -591.49\n",
      "No. Observations:                 173   AIC:                             1193.\n",
      "Df Residuals:                     168   BIC:                             1209.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          40.7478      4.871      8.365      0.000      31.131      50.365\n",
      "prtystrA            0.2235      0.072      3.120      0.002       0.082       0.365\n",
      "np.log(expendA)     5.9704      0.388     15.395      0.000       5.205       6.736\n",
      "democA              3.1566      1.349      2.341      0.020       0.494       5.819\n",
      "np.log(expendB)    -6.6854      0.416    -16.086      0.000      -7.506      -5.865\n",
      "==============================================================================\n",
      "Omnibus:                        3.324   Durbin-Watson:                   1.546\n",
      "Prob(Omnibus):                  0.190   Jarque-Bera (JB):                3.029\n",
      "Skew:                           0.321   Prob(JB):                        0.220\n",
      "Kurtosis:                       3.090   Cond. No.                         486.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Run initial OLS model to get residuals\n",
    "initial_model = smf.ols('voteA ~ prtystrA + np.log(expendA) + democA + np.log(expendB)', data=df4).fit()\n",
    "residuals = initial_model.resid\n",
    "\n",
    "# Step 2: Estimate the variance function (e.g., regressing residuals on predictors)\n",
    "# We regress the log of squared residuals to estimate the heteroskedasticity form\n",
    "df4['log_resid_sq'] = np.log(residuals**2)\n",
    "variance_model = smf.ols('log_resid_sq ~ prtystrA + np.log(expendA) + democA + np.log(expendB)', data=df4).fit()\n",
    "\n",
    "# Step 3: Use fitted values from the variance model as weights in the WLS regression\n",
    "weights = np.exp(variance_model.fittedvalues)\n",
    "model_vote_fgls = smf.wls('voteA ~ prtystrA + np.log(expendA) + democA + np.log(expendB)', data=df4, weights=1/weights).fit()\n",
    "print(model_vote_fgls.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce6f9e",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "#### Coefficients:\n",
    "\n",
    "The coefficients are slightly different compared to the original OLS and robust standard errors model. For example:\n",
    "The coefficient for Intercept is 40.7478 in the WLS model, which is higher than in the original OLS and robust models.\n",
    "Other coefficients, such as np.log(expendA) and np.log(expendB), also show slight variations.\n",
    "\n",
    "#### Standard Errors:\n",
    "The standard errors are generally smaller in the WLS model compared to the robust standard error model, suggesting that the weights applied in WLS help stabilize the variance, reducing heteroskedasticity's impact on the estimates.\n",
    "\n",
    "#### Significance Levels:\n",
    "All variables remain significant at conventional levels (5% or better), and there are no changes in terms of significance status compared to the original models. However, due to smaller standard errors, the t-values are generally higher, enhancing the statistical strength of the findings.\n",
    "\n",
    "#### Why These Changes?\n",
    "\n",
    "WLS adjusts for heteroskedasticity by weighting each observation based on the inverse of the variance of its errors, which helps reduce the influence of observations with larger variance. This leads to more precise estimates, which often have smaller standard errors than OLS with robust standard errors.\n",
    "\n",
    "The small changes in coefficients suggest that some observations with high variance in the residuals had disproportionate influence in the OLS model, which is mitigated here.\n",
    "\n",
    "#### Conclusion:\n",
    "The WLS model is likely more reliable than the OLS model in the presence of heteroskedasticity, as it directly addresses this issue through weighting, leading to more stable standard errors and potentially more efficient coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62433b2",
   "metadata": {},
   "source": [
    "### Comparison of Standard Errors:\n",
    "\n",
    "In the original OLS model, the standard errors were unadjusted for heteroskedasticity, which can lead to unreliable inference when heteroskedasticity is present.\n",
    "\n",
    "In the model with heteroskedasticity-robust standard errors (HC0), the standard errors were adjusted to be more reliable in the presence of heteroskedasticity, and they tended to be slightly larger than the unadjusted ones.\n",
    "\n",
    "In the WLS model, the standard errors are generally smaller than those in the robust model, indicating that WLS is effectively accounting for the heteroskedasticity by applying weights, which stabilizes the variance and reduces the uncertainty in the estimates.\n",
    "\n",
    "#### Significance Levels:\n",
    "\n",
    "Across all models, the significance of each predictor remained largely consistent. All key variables remained statistically significant, suggesting that heteroskedasticity did not drastically affect the p-values in this case.\n",
    "\n",
    "However, the WLS model yielded higher t-values due to the smaller standard errors, strengthening the statistical significance of each predictor.\n",
    "\n",
    "#### Changes in Coefficients:\n",
    "\n",
    "The coefficients showed only slight changes across the models, with the WLS model’s coefficients differing slightly from the original OLS and robust models. This indicates that heteroskedasticity had a minor effect on the point estimates but impacted the standard errors more significantly.\n",
    "\n",
    "For example, the intercept in the WLS model was higher than in the other models, suggesting that some high-variance observations influenced the intercept in the OLS model.\n",
    "\n",
    "#### Explanation for Observed Differences:\n",
    "\n",
    "The differences observed are due to the different ways these models handle heteroskedasticity. The robust standard errors model adjusts the standard errors to account for heteroskedasticity but does not change the point estimates.\n",
    "The WLS model, on the other hand, re-weights observations based on the estimated variance of the residuals, which can affect both the coefficients and standard errors, providing more efficient estimates in the presence of heteroskedasticity.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "The WLS model is preferred in this context because it directly addresses heteroskedasticity through weighting, leading to smaller and more stable standard errors, which strengthens the inference. This model would likely yield the most reliable results for decision-making in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c53bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
